{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "",
   "id": "b1697f57eb706aae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##\n",
    "\n",
    "## Hardware-Level Caching\n",
    "\n",
    "### Leveraging the Chip Layer Caches\n",
    "- Big Cache, sure... L1, L2, L3\n",
    "- Choosing your hardware purposefully, eg EPYC MegaL3* Cache vs Xeon MAX**\n",
    "\n",
    "[*] Not its formal marketing name, but close\n",
    "[**] This is its actual marketing name\n",
    "\n",
    "### DRAM Faster All The Time\n",
    "DDR4 no more.. we go with DDR5 and soon DDR6.\n",
    "\n",
    "#### Concerns for DRAM\n",
    "- NUMA, and it applies to chiplet designs just as much as it applies to multi-socket motherboards.\n",
    "- Interleaved memory: and how it can be used to improve performance, improve high-availability, as well as concerns about degrading performance if the design does not fit the requirements.\n",
    "- Memory channel density: this can increase total DRAM density, though in some architectures there is a DRAM speed penalty for populating more than 1 DIMM per Channel (1DPC).\n",
    "\n",
    "\n",
    "### NVMe are not all equal\n",
    "- U.2, U.3, AIC, or Rulers? Different specs all around, and their interconnects to the PCIe bus make a substantial difference.\n",
    "- SLC on there, is it an SLC cache or is the full NAND array made of SLC\n",
    "\n",
    "### RDMA and Network Shared Memory\n",
    "Is OpenMPI ever enough? Is RDMA what we want? Why not Myrcom\n",
    "\n",
    "#### Interconnect Designs for Memory Nets\n",
    "As a starting point, the Torus has been a highly performing base architecture for Top-500 supercomputers. More on that here:\n",
    "- https://en.wikipedia.org/wiki/Torus_interconnect#Performance\n",
    "\n",
    "Variations on the theme, including SeaStar and Gemini interconnects are described here:\n",
    "- https://github.com/jeffhammond/HPCInfo/blob/master/docs/Cray.md\n",
    "\n",
    "#### Memory Passing Interface - aka MPI\n",
    "What would be do with a shared memory network if not pass data back and forth between compute nodes? Yes, and in a standardized manner, where MPI offers plenty of examples. This has been an evolving field of computer science, which deserves a variety of source code examples and tutorials:\n",
    "- From the great Jeff Hammond: https://github.com/jeffhammond/HPCInfo/tree/master/mpi\n",
    "- Lawrence Livermore Labs gets a mention: https://hpc-tutorials.llnl.gov/mpi/\n",
    "\n",
    "\n",
    "### Multi-Node Distributed Caching\n",
    "This necessarily involves an amalgam of the various cache layers and technologies, and so we'll simply consider this to be the default when dealing with clusters of KVUs. The exceptions include hypothetical designs like the following:\n",
    "\n",
    "- Micro-Cluster: A single CS-N with 2-4U compute nodes, vertically scaled for high-density resources per node, leveraging 10s-100s of VMs per single-node.\n",
    "    - Each single physical node could be deployed as a single dedicated tenant, similar in concept to using single-physical L2 network domains for dis-aggregation and strong physical isolation of systems when logical partitioning is not relevant by CSO requirements or DoD spec or similar architectural design requirement.\n",
    "    - In this design the VMs could participate in aggregation-level distributed caching internally to the physical node, but would not leverage hardware offloads directly on NICs or DPUs unless distributed via SR-IOV or NPAR style controls within the single-node's OS and kernel + CGroups security layers.\n",
    "\n",
    "\n",
    "## Application Level Caching\n",
    "\n",
    "\n",
    "### Semantics-Schamantics with these LLM Queryings\n",
    "\n",
    "\n",
    "### Library Layer Cache Optimizers\n",
    "\n",
    "#### Template and Prefix Caching\n",
    "#### Multi-Turn Conversions\n",
    "#### RAG and Top-K Concerns\n",
    "#### Vector Embeds, aka The Matryoshka Method\n",
    "#### SDK Optimizers Per-Platform\n",
    "\n",
    "\n",
    "## The Many Methods of Popularized Cache Algorithms\n",
    "\n",
    "- Most Recently Used (MRU): evicts the most recently used item, which can be useful in page-scanning engines or cyclical-control loop caches. This operates under the assumption that the MRU item was scanned and accessed, but then will not be referenced again until the next full-loop or full-cycle/page. It is useful in very specific circumstances, or as a layered algo with trigger calls to enable/disable its functionality as-needed.\n",
    "\n",
    ". MRU is only beneficial in specific patterns (like scanning cyclical patterns). It’s generally not used as a primary policy in caching systems for LLMs, except possibly in some layered buffer situations.\n",
    "\n",
    "- Least Recently Used (LRU): possibly the most commonly seen cache algo in the wild, LRU evicts items accessed the longest time ago, operating via the assumption that if it hasn’t been used in a while then it’s less likely to be needed soon. Downside: LRU only tracks recency, not frequency of use, and has no weighting controls.\n",
    "\n",
    "- LRU variants (2Q, LRU/K, etc):\n",
    "  - 2Q is a strategy that introduces a probationary queue and a protected queue\n",
    "  - LRU/K and LRU/2 track the k-th most recent access rather than the last access when making eviction decisions\n",
    "\n",
    "- Least Frequently Used (LFU): evicts the item with the lowest access count to better capture long-term hot items vs cold items, another entry in the list of comparatively simple cache algos.\n",
    "\n",
    "- Random: as it stands, it's A method, but not an often-best method for cache controls.\n",
    "\n",
    "- FIFO: a circular buffer type cache policy, similar to Round-Robin method in Load-Balancing controls. Very simple, and sometimes KISS is all that's needed.\n",
    "\n",
    "- Clock Second Chance (CSCh): An approximation of LRU used in OS kernel page replacement, which arranges pages in a circular buffer like a clock and gives a \"second chance\" to pages that have been accessed recently by marking them before eviction. Effectively this is a Round-Robin method with a simple weighting system.\n",
    "\n",
    "- Adaptive Replacement Cache (ARC): ARC is a highly regarded algorithm that adapts between LRU and LFU dynamically, algorithmically defined via tunables, used in the ZFS filesystem's DRAM and L2 data segments, as well as various applications which require real-time analysis and automated tuning of the caching system. This is the most advanced of the cache controls described here.\n",
    "\n",
    "- Segmented LRU (S/LRU): Takes the LRU method and splits cache entries into segments, referred to as \"protected vs probationary\".\n",
    "\n",
    "- Window TinyLFU (Wt/LFU): an advancement upon the standard LFU policy which uses a small LRU \"windowing controller\" and adapts the concept of \"TinyLFU\" for its cache admission policy design. These actions can be modeled by ARC and improved upon dynamically, where-as the non-ARC methods of Wt/LFU may become stale over time or induce inefficiency which is difficult to change during production use - unless real-time controls are implemented.\n",
    "\n",
    "\n",
    "## Cache invalidation strategies:\n",
    "- Common strategies include time-to-live (TTL)\n",
    "\n",
    "\n",
    "\n",
    "## Concerns in Caching Architectures"
   ],
   "id": "ae49f5ecf9a84a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e67705f3cdb715b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
